---
title: 'STAT 5014: Homework 7'
author: "Bobby Soule"
date: "10/24/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(parallel)
library(foreach)
library(doParallel)
library(doRNG)
```

This week, we spoke about parallelizing R.  We looked at two methods, parallizing for loops and parallelizing the apply family of functions.  In this homework, we will explore parallelization.  A word of caution, this homework involves parallel functions.  If you try to start a cluster when one is already started, you can easily lock up your computer and have to do a hard boot.  A. Save (commit) your homework often. B. Remember to stop the cluster before starting another!!  C. Figure out how many cores your computer has to play with and back off by 1. IE, mine has 8, I prefer to not exceed cluster sizes of 6 or 7.

## Problem 2: Sums of Squares

Similar to the last homework, we will calculate sums of squares total using:

a. a for loop to iterate through all data points calculating the summed squared difference between the data points and mean of the data.

b. repeat part a, but use vector operations to effect the same computation

c. repeat part a, but use dopar

d. repeat part a, but use parSapply

In all cases, wrap the code in "system.time({})".  You should report the final answer and timings in a nice table.  Make note of any parameters you had to set.  What observations do you have?

**Note: I have decreased the size of n in problem two so that my code runs in a reasonable amount of time.**

```{r ss, echo=T, eval=T, include=T}

set.seed(12345)
y <- rnorm(n = 1e5, mean = 1, sd = 1)
n <- length(y)
ybar <- mean(y)

sum_sq <- list()
time <- list()

# Part A
time[["a"]] <- system.time({
  sum_sq[["a"]] <- 0
  for (i in 1:n) {
    sq_dev <- (y[i] - ybar) ^ 2
    sum_sq$a <- sum_sq$a + sq_dev
  }
})

# Part B
time[["b"]] <- system.time({
  y_dev <- y - ybar
  sum_sq[["b"]] <- t(y_dev) %*% y_dev
})

# Part C
cl <- makeCluster(3)
registerDoParallel(cl)
time[["c"]] <- system.time({
  sq_dev <- foreach(i = 1:n, .combine='c') %dopar% {
    (y[i] - ybar) ^ 2
  }
  sum_sq[["c"]] <- sum(sq_dev)
})
stopCluster(cl)

# Part D
cl <- makeCluster(3)
clusterExport(cl, c("y", "ybar"))
time[["d"]] <- system.time({
  sq_dev <- parSapply(cl, 1:n, function(i) (y[i] - ybar)^2)
  sum_sq[["d"]] <- sum(sq_dev)
})
stopCluster(cl)
```

In this problem, the fastest method was the one used in part b that relied on vector operations. The two methods that used parrallel computing were actually the slowest methods, which was surprising to me. In class, I recall that the parallel methods outperformed regular for loops. Using the foreach and dopar functions in part c took so long to run that I had to reduce the size of n. While the parSapply function still ran in a reasonable amount of time, it still took longer than the regular for loop and vector operations. Perhaps I have not correctly implemented parallelization; do you see anything I could change to make the parallelized methods faster?

```{r, echo=F}
results2 <- cbind(SS = sum_sq, Time = lapply(time, `[[`, 3))
knitr::kable(results2, caption = "Sum of Squares Calculation Times")
```

## Problem 3: Gradient Descent

From the last homework, the algorithm is:

\begin{itemize}
    \item $while(abs(\Theta_0^{i}-\Theta_0^{i-1}) \text{ AND } abs(\Theta_1^{i}-\Theta_1^{i-1}) > tolerance) \text{ \{ }$
    \begin{eqnarray*}
        \Theta_0^i &=& \Theta_0^{i-1} - \alpha\frac{1}{m}\sum_{i=1}^{m} (h_0(x_i) -y_i)  \\
        \Theta_1^i &=& \Theta_1^{i-1} - \alpha\frac{1}{m}\sum_{i=1}^{m} ((h_0(x_i) -y_i)x_i) 
    \end{eqnarray*}
    $\text{ \} }$
\end{itemize}

Where $h_0(x) = \Theta_0 + \Theta_1x$.  

What would you parallelize around here?  Hint: what parameters do YOU need to specify.

Given $X$ and $\vec{h}$ below, implement the above algorithm, parallelizing around the hint given above.  Compare the values obtained and contrast those with the results given by lm(h~0+$X$).

```{r eval=F, echo=T, include=T}
set.seed(1256)
theta <- as.matrix(c(1, 2), nrow=2)
X <- cbind(1, rep(1:10,10))
y <- X %*% theta + rnorm(100, 0, 0.2)

tol <- 0.001
alpha <- 0.01
m <- length(y)
d_theta <- c()

continue = TRUE
while (continue) {
  
  d_theta[1] <- (alpha / m) * sum((theta[1] + theta[2] * X[, 2]) - y)
  d_theta[2] <- (alpha / m) * sum(((theta[1] + theta[2] * X[, 2]) - y) * X[, 2])
  
  theta <- theta - d_theta
  
  if (abs(d_theta[1]) > tol & abs(d_theta[2]) > tol) {
    continue = TRUE
  } else {continue = FALSE}
  
}
theta
```

I could not figure out how to parallelize this loop since the nth iteration is dependent on the result from the (n-1)th iteration. I was also not able to find a paralleized version of the while loop after doing some searching on google. Any recommendaitons?

## Problem 4: Bootstrapped Regression

There are situations where you want another realization of the data from a population.  If you have a random sample from a population, you can randomly draw from that sample (with replacement) to produce a new realization of the sample.  If you do this many times calculating some sort of summary statistic for each bootstrapped sample, this is called bootstrapping.

The basic procedure for bootstrapping in the regression setting is:

For b $\in$ {1,$\dots$,B}
\begin{itemize}
    \item Sample $Z^{(b)} = (X,Y)^{(b)}$  
    \item Calculate $\hat\beta^{(b)}$  
\end{itemize}

A. Impliment this algorithm using the data generated below for B=10,000.  Do not use the boot package, use the sample function in base R: sample(x=,size=,replace=T).  
B. Create a table of the result with the appropriate summary statistics.  
C. Create histograms of the distribution of $\hat\beta$'s.

Which parallelization method did you use?  What impediments did you encounter?  How long did it take?

```{r boot_regression_data, eval=T, include=T, echo=T}
set.seed(1267)
n <- 200
X <- 1/cbind(1,rt(n,df=1),rt(n,df=1),rt(n,df=1))
beta <- c(1,2,3,0)
Y <- X %*% beta + rnorm(100,sd=3)
B <- 100

cl <- makeCluster(3)
registerDoParallel(cl)
time <- system.time({
  beta_est <- foreach(b = 1:B, .combine='cbind') %dorng% {
    id <- sample(x = 1:n, size = n, replace = T)
    y <- Y[id]
    x <- X[id, -1]
    coef(lm(y ~ x))
  }
})
stopCluster(cl)
```

In this problem, I once again used the foreach function to parrallelize the bootstrap process In place of the dopar operator, however, I used dorng. This ensures that the loop is reproducible, which is necessary since we are generating random numbers. Using parrallelization, the for loop took:

```{r, echo=F}
paste(unname(time[3]), "seconds")
```

Below is a table of summary statistics and a histogram for each coefficient. All of the bootstrapped coefficients had means very close to their true values, except for the intercept. The true value of the intercept was set to one, but the center of the distribution of the estimated intercepts is closer to 1.2. Since the standard deviation of the estimated intercepts is close to 0.2, it seems reasonable that such a deviation from the true intercept could have been random chance. I did not encounter any other issues.

```{r, echo=F}
results4 <- apply(beta_est, 1, function(x) c(min = min(x), mean = mean(x),
                                             max = max(x), sd = sd(x)))
colnames(results4) <- c("beta0", "beta1", "beta2", "beta3")
knitr::kable(results4, caption = "Summary Statistics of Bootstrapped Coefficients")

par(mfrow=c(2,2))
hist(beta_est[1,], main = "Distribution of Intercept Estimates", xlab = "Beta0")
hist(beta_est[2,], main = "Distribution of Beta1 Estimates", xlab = "Beta1")
hist(beta_est[3,], main = "Distribution of Beta2 Estimates", xlab = "Beta2")
hist(beta_est[4,], main = "Distribution of Beta3 Estimates", xlab = "Beta3")
```
